require "ruby_neural_nets/models/onnx_torch.#{RUBY_PLATFORM}"

describe RubyNeuralNets::Models::OnnxTorch do

  # Setup a models path containing an ONNX model for a 1 layer model
  #
  # Parameters::
  # * Proc: Code block called with the models path created
  #   * Parameters::
  #     * *models_path* (String): The models directory
  def with_one_layer_model
    with_test_dir(
      # Generate ONNX content for 110x110x3 images to be classified to 3 classes
      'test_one_layer_model.onnx' => onnx_one_layer(110, 110, 3, 3)
    ) do |models_path|
      yield models_path
    end
  end

  describe '#stats' do

    it 'returns the right model statistics' do
      with_one_layer_model do |models_path|
        # Setup test data: create a model with specific dimensions (110x110x3 images on 3 classes)
        # Based on the one_layer.onnx model generated by the bin/generate_onnx script
        expect(described_class.new(110, 110, 3, 3, onnx_model: 'test_one_layer_model', models_path:).stats).to eq(
          {
            parameters: {
              'dense_weight' => { size: 108900 }, # 3 classes * (110*110*3) features
              'dense_bias' => { size: 3 }          # 3 classes bias
            }
          }
        )
        end
    end

  end

  describe '#forward_propagate' do

    it 'produces expected output values for given input using ONNX model' do
      with_one_layer_model do |models_path|
        # Setup test data: create a model with specific dimensions (110x110x3 images on 3 classes)
        # Based on the one_layer.onnx model generated by the bin/generate_onnx script
        model = described_class.new(110, 110, 3, 3, onnx_model: 'test_one_layer_model', models_path:)

        # Input: simple flattened images, shape [batch_size, n_x] = [2, 36300] for 2 samples
        x = ::Torch.rand(2, 110 * 110 * 3, dtype: :double)

        # Call forward_propagate
        output = model.forward_propagate(x)

        # Assert output shape
        expect(output.shape).to eq([2, 3])
        # Values should be reasonable (probabilities after softmax, should sum to 1)
        expect(output.numo.to_a.flatten.all? { |v| v.is_a?(Float) }).to be true
        
        # Check that softmax output sums to approximately 1 for each sample
        output_sums = output.sum(dim: 1).numo.to_a
        output_sums.each do |sum|
          expect(sum).to be_within(0.001).of(1.0)
        end
      end
    end

    it 'handles single sample input correctly' do
      with_one_layer_model do |models_path|
        # Setup test data: create a model with specific dimensions (110x110x3 images on 3 classes)
        # Based on the one_layer.onnx model generated by the bin/generate_onnx script
        model = described_class.new(110, 110, 3, 3, onnx_model: 'test_one_layer_model', models_path:)

        # Input: single flattened image, shape [batch_size, n_x] = [1, 36300]
        x = ::Torch.rand(1, 110 * 110 * 3, dtype: :double)

        # Call forward_propagate
        output = model.forward_propagate(x)

        # Assert output shape
        expect(output.shape).to eq([1, 3])
        # Values should be reasonable (probabilities after softmax)
        expect(output.numo.to_a.flatten.all? { |v| v.is_a?(Float) }).to be true
        
        # Check that softmax output sums to approximately 1
        output_sum = output.sum.item
        expect(output_sum).to be_within(0.001).of(1.0)
      end
    end

  end

  describe '#gradient_descent' do

    it 'updates parameters correctly during gradient descent' do
      with_one_layer_model do |models_path|
        # Setup test data: create a model with specific dimensions (110x110x3 images on 3 classes)
        # Based on the one_layer.onnx model generated by the bin/generate_onnx script
        model = described_class.new(110, 110, 3, 3, onnx_model: 'test_one_layer_model', models_path:)

        # Input: flattened images, shape [batch_size, n_x] = [2, 36300]
        x = ::Torch.rand(2, 110 * 110 * 3, dtype: :double)

        # Labels: class indices for CrossEntropyLoss
        y = ::Torch.tensor([0, 1], dtype: :long)

        # Setup optimizer
        optimizer = ::Torch::Optim::Adam.new(model.parameters.map(&:torch_parameter), lr: 0.1)

        # Store original parameter values
        original_params = model.parameters.map { |p| [p.name, p.torch_parameter.numo.dup] }.to_h

        # Forward propagate
        model.initialize_back_propagation_cache
        a = model.forward_propagate(x, train: true)

        # Compute loss
        criterion = ::Torch::NN::CrossEntropyLoss.new
        loss = criterion.call(a, y)

        # Backward propagate
        model.gradient_descent(nil, a, nil, loss)

        # Check that gradients were computed by autograd
        model.parameters.each do |param|
          expect(param.torch_parameter.grad).not_to be_nil
          expect(param.torch_parameter.grad.numo.to_a.flatten.any? { |g| g != 0 }).to be true
        end

        # Update parameters
        optimizer.step

        # Assert that parameters have been updated
        model.parameters.each do |param|
          expect(param.torch_parameter.numo).not_to eq(original_params[param.name])
        end
      end
    end

  end

end
