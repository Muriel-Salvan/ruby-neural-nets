#!/usr/bin/env ruby

# TODO: Find why do I need that?
$LOAD_PATH.unshift(File.expand_path("#{__dir__}/../lib"))

# Make it easy to debug
require 'byebug'

# Get all options from CLI
require 'ruby_neural_nets/options'
options = RubyNeuralNets::Options.new
options.parse_cli

# Set debug mode for Logger
require 'ruby_neural_nets/logger'
RubyNeuralNets::Logger.debug_mode = options[:debug]

require 'numo/gnuplot'
require 'numo/linalg/linalg'
require 'numo/narray'
# Load a C extension for efficient computations
Numo::Linalg::Loader.load_openblas(ENV['OPEN_BLAS_PATH']) if RUBY_PLATFORM == 'x64-mingw-ucrt' && !ENV['OPEN_BLAS_PATH'].nil?

require 'ruby_neural_nets/helpers'
RubyNeuralNets::Helpers.init(
  model_seed: options[:model_seed],
  instability_checks: options[:instability_checks]
)

# Create the progress tracker for all experiments
require 'ruby_neural_nets/progress_tracker'
progress_tracker = RubyNeuralNets::ProgressTracker.new(display_graphs: options[:display_graphs])

# Return a unique experiment id, taking into account previously instantiated experiments
#
# Parameters::
# * *experiment_id* (String): Wishful experiment id
# * *experiments* (Array<Experiment>): Existing experiments
# Result::
# * String: Unique experiment id
def unique_experiment_id(experiment_id, experiments)
  exp_id_candidate = experiment_id
  exp_id_idx = 0
  while experiments.find { |select_exp| select_exp.exp_id == exp_id_candidate }
    exp_id_candidate = "#{experiment_id}_#{exp_id_idx}"
    exp_id_idx += 1
  end
  exp_id_candidate
end

# Instantiate all experiments
require 'ruby_neural_nets/experiment'
require 'ruby_neural_nets/gradient_checker'
require 'ruby_neural_nets/profiler'
experiments = []
options.experiments.each do |exp_info|
  exp_info[:training_times][:value].times.each do |train_idx|
    # Load data from the dataset
    data_loader = options.instantiate(exp_info[:data_loader])
    dataset_training = data_loader.dataset(:training)

    # Display some stats
    puts "===== Preparing experiment #{exp_info[:exp_id][:value]}:"
    labels = data_loader.labels
    data_loader.display_stats
    # data_loader.display_sample(:training, labels.first)
    image_stats = data_loader.image_stats
    puts "Images size: #{image_stats[:rows]} x #{image_stats[:cols]} x #{image_stats[:channels]}"
    puts

    # Create the model
    model = options.instantiate(exp_info[:model], image_stats[:rows], image_stats[:cols], image_stats[:channels], labels.size)
    require 'json'
    puts "Model statistics:\n#{JSON.pretty_generate(model.stats)}"

    # Define the accuracy measure and loss
    accuracy = options.instantiate(exp_info[:accuracy])
    loss = options.instantiate(exp_info[:loss], weight_decay: exp_info[:optimizer][:options][:weight_decay][:value])

    optimizer = options.instantiate(exp_info[:optimizer])
    optimizer.teach_parameters(model.parameters)
    nbr_epochs = exp_info[:nbr_epochs][:value]
    gradient_checker = RubyNeuralNets::GradientChecker.new(gradient_checks: exp_info[:gradient_checks][:value])
    gradient_checker.link_to_model(model, loss)
    profiler = RubyNeuralNets::Profiler.new(profiling: exp_info[:profiling][:value])

    # Create the experiments
    # First the dev one that will be used before parameters are being trained, so that curves of the same X coordinate in graphs match models with the same parameters.
    dev_experiment = nil
    if exp_info[:eval_dev][:value]
      dev_experiment = RubyNeuralNets::Experiment.new(
        exp_id: unique_experiment_id("#{exp_info[:exp_id][:value]}_dev", experiments),
        dataset: data_loader.dataset(:dev),
        training_mode: false,
        accuracy:,
        data_loader:,
        loss:,
        model:,
        optimizer:,
        nbr_epochs:,
        gradient_checker:,
        profiler:,
        display_units: {},
        display_samples: exp_info[:display_samples][:value],
        dev_experiment: nil,
        dump_minibatches: exp_info[:dump_minibatches][:value]
      )
      experiments << dev_experiment
    end
    experiments << RubyNeuralNets::Experiment.new(
      exp_id: unique_experiment_id("#{exp_info[:exp_id][:value]}_training", experiments),
      dataset: dataset_training,
      training_mode: true,
      accuracy:,
      data_loader:,
      loss:,
      model:,
      optimizer:,
      nbr_epochs:,
      gradient_checker:,
      profiler:,
      display_units: exp_info[:track_layer][:value].to_h,
      display_samples: exp_info[:display_samples][:value],
      dev_experiment: dev_experiment,
      dump_minibatches: exp_info[:dump_minibatches][:value],
      early_stopping_patience: exp_info[:early_stopping_patience][:value]
    )
  end
end

# Track the progress of all experiments
experiments.each { |experiment| progress_tracker.track(experiment) }

require 'ruby_neural_nets/trainer'
RubyNeuralNets::Trainer.new(progress_tracker:).train(experiments)

# Close graphs
progress_tracker.close_graphs
